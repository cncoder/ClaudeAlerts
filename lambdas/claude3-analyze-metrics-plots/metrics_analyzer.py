import json
import urllib.request
import urllib.error
import logging
import boto3
import time
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger()

class Config:
    """é…ç½®ç±»"""
    # APIé…ç½®
    API_HOST = "10.XX.30.2XX"
    DIFY_ENDPOINT = f"http://{API_HOST}/v1/workflows/run"
    DIFY_API_KEY = "app-ePf7XXXXXN99NdN"
    DIFY_TIMEOUT = 60  # APIè°ƒç”¨è¶…æ—¶æ—¶é—´(ç§’)
    DIFY_MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
    DIFY_RETRY_DELAY = 5  # é‡è¯•é—´éš”(ç§’)
    
    # Larké…ç½®
    LARK_WEBHOOK = "https://open.larksuite.com/open-apis/bot/v2/hook/477XXXXXX4ab5"
    LARK_TIMEOUT = 10
    
    # S3é…ç½®
    S3_URL_EXPIRY = 3600
    
    # æŒ‡æ ‡é…ç½®
    METRICS_CONFIG = {
        'cpu': {'name': 'CPU', 'unit': '%', 'color': 'blue'},
        'memory': {'name': 'Memory', 'unit': '%', 'color': 'purple'},
        'network': {'name': 'Network', 'unit': 'Mbps', 'color': 'grey'}
    }
    
    # å¤„ç†é…ç½®
    MIN_REMAINING_TIME = 30

    # å¼‚å¸¸çº§åˆ«é…ç½®
    SEVERITY_LEVELS = {
        'critical': {
            'keywords': ['ä¸¥é‡', 'å±é™©', 'å¼‚å¸¸'],
            'icon': 'ğŸ”´',
            'color': 'red',
            'title_color': 'red'
        },
        'warning': {
            'keywords': ['è­¦å‘Š', 'æ³¢åŠ¨', 'åç¦»'],
            'icon': 'âš ï¸',
            'color': 'orange',
            'title_color': 'yellow'
        },
        'info': {
            'keywords': ['æç¤º', 'å»ºè®®'],
            'icon': 'â„¹ï¸',
            'color': 'blue',
            'title_color': 'blue'
        }
    }

class DifyAPIError(Exception):
    """Dify APIè°ƒç”¨å¼‚å¸¸"""
    pass

class S3Client:
    """S3æ“ä½œå®¢æˆ·ç«¯"""
    def __init__(self):
        self.client = boto3.client('s3')
        
    def get_presigned_url(self, bucket: str, key: str) -> str:
        """ç”Ÿæˆé¢„ç­¾åURL"""
        try:
            logger.info(f"ç”ŸæˆS3é¢„ç­¾åURL - Bucket: {bucket}, Key: {key}")
            url = self.client.generate_presigned_url(
                'get_object',
                Params={'Bucket': bucket, 'Key': key},
                ExpiresIn=Config.S3_URL_EXPIRY
            )
            logger.info(f"ç”Ÿæˆé¢„ç­¾åURLæˆåŠŸ")
            return url
        except Exception as e:
            logger.error(f"ç”Ÿæˆé¢„ç­¾åURLå¤±è´¥: {str(e)}")
            raise

    @staticmethod
    def parse_s3_url(s3_url: str) -> Tuple[str, str]:
        """è§£æS3 URL"""
        try:
            logger.debug(f"è§£æS3 URL: {s3_url}")
            parsed = urlparse(s3_url)
            bucket = parsed.netloc
            key = parsed.path.lstrip('/')
            return bucket, key
        except Exception as e:
            logger.error(f"è§£æS3 URLå¤±è´¥: {str(e)}")
            raise

def get_metric_type(source_file: str) -> Optional[str]:
    """ä»æ–‡ä»¶åè·å–æŒ‡æ ‡ç±»å‹"""
    filename = source_file.lower()
    
    if 'cpu' in filename:
        return 'cpu'
    elif 'memory' in filename:
        return 'memory'
    elif 'network' in filename:
        return 'network'
    return None

def make_http_request(url: str, method: str, headers: Dict, data: Optional[Dict] = None, timeout: int = 30) -> Dict:
    """é€šç”¨HTTPè¯·æ±‚å‡½æ•°"""
    try:
        logger.info(f"å‘èµ·HTTPè¯·æ±‚ - URL: {url}, Method: {method}")
        
        request = urllib.request.Request(
            url,
            data=json.dumps(data).encode('utf-8') if data else None,
            headers=headers,
            method=method
        )
        
        with urllib.request.urlopen(request, timeout=timeout) as response:
            response_data = response.read()
            logger.info(f"è¯·æ±‚æˆåŠŸ - çŠ¶æ€ç : {response.status}")
            try:
                return json.loads(response_data)
            except json.JSONDecodeError:
                logger.error(f"å“åº”æ•°æ®è§£æå¤±è´¥: {response_data}")
                raise
                
    except urllib.error.HTTPError as e:
        logger.error(f"HTTPè¯·æ±‚å¤±è´¥: {e.code} {e.reason}")
        logger.error(f"å“åº”å†…å®¹: {e.read().decode('utf-8')}")
        raise
    except urllib.error.URLError as e:
        logger.error(f"URLé”™è¯¯: {str(e)}")
        raise
    except Exception as e:
        logger.error(f"è¯·æ±‚å¼‚å¸¸: {str(e)}")
        raise

class DifyClient:
    """Dify APIå®¢æˆ·ç«¯"""
    def __init__(self):
        self.endpoint = Config.DIFY_ENDPOINT
        self.api_key = Config.DIFY_API_KEY
        self.s3_client = S3Client()

    def _call_dify_api(self, payload: Dict) -> Dict:
        """è°ƒç”¨Dify APIå¹¶å¤„ç†é‡è¯•"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        for attempt in range(Config.DIFY_MAX_RETRIES):
            try:
                logger.info(f"è°ƒç”¨Dify API (å°è¯• {attempt + 1}/{Config.DIFY_MAX_RETRIES})")
                response = make_http_request(
                    url=self.endpoint,
                    method='POST',
                    headers=headers,
                    data=payload,
                    timeout=Config.DIFY_TIMEOUT
                )
                
                # éªŒè¯å“åº”æ•°æ®
                if not isinstance(response, dict):
                    raise DifyAPIError(f"éé¢„æœŸçš„å“åº”ç±»å‹: {type(response)}")
                
                # æ£€æŸ¥dataå­—æ®µ
                if 'data' not in response:
                    raise DifyAPIError(f"å“åº”ç¼ºå°‘dataå­—æ®µ: {response}")
                    
                data = response['data']
                
                # æ£€æŸ¥outputså­—æ®µ
                if 'outputs' not in data:
                    raise DifyAPIError(f"dataç¼ºå°‘outputså­—æ®µ: {data}")
                    
                outputs = data['outputs']
                
                # æ£€æŸ¥resultå­—æ®µ
                if 'result' not in outputs:
                    raise DifyAPIError(f"outputsç¼ºå°‘resultå­—æ®µ: {outputs}")
                
                # åˆ¤æ–­æ˜¯å¦æ— å¼‚å¸¸
                result_xml = outputs.get('result', '')
                if 'æ— å¼‚å¸¸' in result_xml:
                    logger.info("æ£€æµ‹ç»“æœ:æ— å¼‚å¸¸")
                    return {
                        'result': result_xml,
                        'has_anomaly': False
                    }

                # æœ‰å¼‚å¸¸æ—¶éªŒè¯xå­—æ®µ
                if 'x' not in outputs:
                    raise DifyAPIError(f"å¼‚å¸¸åˆ†æç¼ºå°‘xå­—æ®µ: {outputs}")
                
                logger.info("æ£€æµ‹ç»“æœ:å‘ç°å¼‚å¸¸")
                return {
                    'result': result_xml,
                    'x': outputs['x'],
                    'has_anomaly': True
                }
                
            except Exception as e:
                logger.error(f"Dify APIè°ƒç”¨å¤±è´¥: {str(e)}")
                if attempt < Config.DIFY_MAX_RETRIES - 1:
                    logger.info(f"ç­‰å¾… {Config.DIFY_RETRY_DELAY} ç§’åé‡è¯•")
                    time.sleep(Config.DIFY_RETRY_DELAY)
                else:
                    raise DifyAPIError(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {str(e)}")

    def parse_analysis_xml(self, result_xml: str, analysis_xml: str) -> dict:
        """è§£æåŸºç¡€ç»“æœå’Œåˆ†æç»“æœXML"""
        try:
            logger.info("å¼€å§‹è§£æXMLæ•°æ®")
            # éªŒè¯XMLæ•°æ®
            if not result_xml or not analysis_xml:
                raise ValueError("XMLæ•°æ®ä¸ºç©º")
            
            # è§£æresult XML
            result_root = ET.fromstring(result_xml)
            pod_name = result_root.find('pod1').text if result_root.find('pod1') is not None else ''
            
            # è§£æanalysis XML 
            analysis_root = ET.fromstring(analysis_xml)
            
            # è§£æå¼‚å¸¸podä¿¡æ¯
            results = []
            for pod in analysis_root.findall('.//anomaly_pods/pod'):
                pod_data = {
                    'name': pod.find('name').text if pod.find('name') is not None else '',
                    'confidence': pod.find('confidence').text if pod.find('confidence') is not None else '',
                    'priority': pod.find('priority').text if pod.find('priority') is not None else '',
                    'probable_cause': pod.find('probable_cause').text if pod.find('probable_cause') is not None else '',
                    'action': [],
                    'command': [],
                    'investigation': []
                }
                
                # è·å–actionåˆ—è¡¨
                action = pod.find('action')
                if action is not None and action.text:
                    pod_data['action'] = [step.strip() for step in action.text.split('\n') if step.strip()]
                    
                # è·å–commandåˆ—è¡¨    
                command = pod.find('command')
                if command is not None and command.text:
                    pod_data['command'] = [cmd.strip() for cmd in command.text.split('\n') if cmd.strip()]
                    
                # è·å–investigationåˆ—è¡¨
                investigation = pod.find('investigation')
                if investigation is not None and investigation.text:
                    pod_data['investigation'] = [step.strip() for step in investigation.text.split('\n') if step.strip()]
                    
                results.append(pod_data)
                
            # è§£æsummaryä¿¡æ¯
            summary = analysis_root.find('.//summary')
            summary_data = {
                'total_pods': summary.find('total_pods').text if summary.find('total_pods') is not None else '0',
                'risk_level': summary.find('risk_level').text if summary.find('risk_level') is not None else '',
                'urgent_actions': []
            }
            
            # è·å–urgent_actionsåˆ—è¡¨
            urgent_actions = summary.find('urgent_actions')
            if urgent_actions is not None and urgent_actions.text:
                summary_data['urgent_actions'] = [action.strip() for action in urgent_actions.text.split('\n') if action.strip()]
            
            return {
                'pod_name': pod_name,
                'pods': results,
                'summary': summary_data
            }

        except ET.ParseError as e:
            logger.error(f"XMLè§£æé”™è¯¯: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"æ•°æ®å¤„ç†é”™è¯¯: {str(e)}")
            raise

    def analyze_plots(self, plots_data: List[Dict], metric_type: str) -> List[Dict]:
        """åˆ†æå›¾è¡¨æ•°æ®"""
        if not plots_data:
            logger.warning("plots_dataä¸ºç©º")
            return []
            
        results = []
        
        for plot in plots_data:
            try:
                logger.info(f"å¤„ç†å›¾è¡¨ - Service: {plot.get('service')}")
                bucket, key = self.s3_client.parse_s3_url(plot['plot_path'])
                presigned_url = self.s3_client.get_presigned_url(bucket, key)
                
                payload = {
                    "inputs": {
                        metric_type: {
                            "type": "image",
                            "transfer_method": "remote_url",
                            "url": presigned_url
                        }
                    },
                    "response_mode": "blocking",
                    "user": "lambda-user"
                }
                
                # è°ƒç”¨Dify API
                api_result = self._call_dify_api(payload)
                
                if not api_result.get('has_anomaly'):
                    # æ— å¼‚å¸¸æƒ…å†µ,è·³è¿‡
                    logger.info(f"Service {plot['service']} æœªå‘ç°å¼‚å¸¸")
                    continue
                
                # æœ‰å¼‚å¸¸æƒ…å†µ
                result_xml = api_result['result']
                analysis_xml = api_result['x']
                
                if result_xml and analysis_xml:
                    analysis = self.parse_analysis_xml(result_xml, analysis_xml)
                    if analysis:
                        results.append({
                            'service': plot['service'],
                            'analysis': analysis,
                            'plot_url': presigned_url
                        })
                    else:
                        logger.warning(f"è·³è¿‡æ— æ•ˆçš„åˆ†æç»“æœ - Service: {plot.get('service')}")
                else:
                    logger.warning(f"Difyè¿”å›çš„XMLæ•°æ®ä¸ºç©º - Service: {plot.get('service')}")

            except Exception as e:
                logger.error(f"å¤„ç†å›¾è¡¨å¤±è´¥ {plot.get('service', 'unknown')}: {str(e)}")

        return results

class LarkBot:
    """Larkæœºå™¨äººå®¢æˆ·ç«¯"""
    def __init__(self):
        self.webhook = Config.LARK_WEBHOOK

    def create_service_card(self, service_result: dict, metric_type: str) -> dict:
        """åˆ›å»ºæœåŠ¡åˆ†æå¡ç‰‡"""
        service_name = service_result['service']
        analysis_data = service_result['analysis']
        plot_url = service_result['plot_url']
        
        # è·å–summaryä¿¡æ¯
        summary = analysis_data['summary']
        risk_level = summary['risk_level']
        
        # ç¡®å®šé£é™©çº§åˆ«é¢œè‰²
        severity_color = 'blue'
        if risk_level == 'é«˜å±':
            severity_color = 'red'
        elif risk_level == 'ä¸­å±':
            severity_color = 'orange'
        
        card = {
            "header": {
                "template": severity_color,
                "title": {
                    "content": f"{service_name} åˆ†æç»“æœ",
                    "tag": "plain_text"
                }
            },
            "elements": [
                # æ·»åŠ summaryä¿¡æ¯
                {
                    "tag": "div",
                    "text": {
                        "content": (
                            f"**é£é™©ç­‰çº§**: {risk_level}\n"
                            f"**å¼‚å¸¸Podæ•°é‡**: {summary['total_pods']}\n"
                            "\n**ç´§æ€¥æªæ–½**:\n" +
                            "\n".join([f"â€¢ {action}" for action in summary['urgent_actions']])
                        ),
                        "tag": "lark_md"
                    }
                },
                {"tag": "hr"}
            ]
        }

        # æ·»åŠ æ¯ä¸ªå¼‚å¸¸podçš„è¯¦ç»†ä¿¡æ¯
        for pod in analysis_data['pods']:
            if pod['name']:  # åªæ·»åŠ æœ‰æ•ˆçš„Podä¿¡æ¯
                pod_info = {
                    "tag": "div",
                    "text": {
                        "content": (
                            f"ğŸ” **Pod**: {pod['name']}\n"
                            f"**ç½®ä¿¡åº¦**: {pod['confidence']} [ğŸ“ŠæŸ¥çœ‹ç›‘æ§]({plot_url})\n"
                            f"**ä¼˜å…ˆçº§**: {pod['priority']}\n"
                            f"**å¯èƒ½åŸå› **: {pod['probable_cause']}"
                        ),
                        "tag": "lark_md"
                    }
                }
                card["elements"].append(pod_info)
                
                # æ·»åŠ æ“ä½œå»ºè®®
                if pod['action']:
                    actions = ["**å»ºè®®æªæ–½**:"]
                    for action in pod['action']:
                        actions.append(f"â€¢ {action}")
                    card["elements"].append({
                        "tag": "div",
                        "text": {
                            "content": "\n".join(actions),
                            "tag": "lark_md"
                        }
                    })
                    
                # æ·»åŠ å‘½ä»¤å»ºè®®    
                if pod['command']:
                    commands = ["**æ¨èå‘½ä»¤**:"]
                    for cmd in pod['command']:
                        commands.append(f"`{cmd}`")
                    card["elements"].append({
                        "tag": "div",
                        "text": {
                            "content": "\n".join(commands),
                            "tag": "lark_md"
                        }
                    })
                    
                # æ·»åŠ è°ƒæŸ¥å»ºè®®
                if pod['investigation']:
                    investigations = ["**è°ƒæŸ¥æ­¥éª¤**:"]
                    for step in pod['investigation']:
                        investigations.append(f"â€¢ {step}")
                    card["elements"].append({
                        "tag": "div",
                        "text": {
                            "content": "\n".join(investigations),
                            "tag": "lark_md"
                        }
                    })
                    
                card["elements"].append({"tag": "hr"})

        return card

    def send_message(self, results: List[Dict], source_csv: str, time_window: int, metric_type: str) -> None:
        """å‘é€æ¶ˆæ¯åˆ°Lark"""
        try:
            # éªŒè¯ç»“æœåˆ—è¡¨
            if not results:
                logger.info("æ²¡æœ‰å¼‚å¸¸ç»“æœ,è·³è¿‡å‘é€æ¶ˆæ¯")
                return
                
            logger.info("å‡†å¤‡å‘é€Larkæ¶ˆæ¯")
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            metric_config = Config.METRICS_CONFIG[metric_type]
            
            # åˆ›å»ºä¸»å¡ç‰‡
            main_card = {
                "config": {
                    "wide_screen_mode": True
                },
                "header": {
                    "template": metric_config['color'],
                    "title": {
                        "content": f"{metric_config['name']}æŒ‡æ ‡åˆ†ææŠ¥å‘Š",
                        "tag": "plain_text"
                    }
                },
                "elements": [
                    {
                        "tag": "div",
                        "text": {
                            "content": (
                                f"**æ—¶é—´**: {current_time}\n"
                                f"**æ•°æ®æº**: {source_csv}\n"
                                f"**åˆ†ææ—¶é—´çª—å£**: {time_window}å°æ—¶"
                            ),
                            "tag": "lark_md"
                        }
                    },
                    {"tag": "hr"}
                ]
            }

            # æ·»åŠ æ¯ä¸ªæœåŠ¡çš„åˆ†æå¡ç‰‡
            for result in results:
                service_card = self.create_service_card(result, metric_type)
                if service_card:
                    main_card["elements"].extend(service_card["elements"])

            message = {
                "msg_type": "interactive",
                "card": main_card
            }
            
            logger.info("å‘é€æ¶ˆæ¯åˆ°Lark")
            response = make_http_request(
                url=self.webhook,
                method='POST',
                headers={'Content-Type': 'application/json'},
                data=message,
                timeout=Config.LARK_TIMEOUT
            )
            
            logger.info("Larkæ¶ˆæ¯å‘é€æˆåŠŸ")

        except Exception as e:
            logger.error(f"å‘é€Larkæ¶ˆæ¯å¤±è´¥: {str(e)}")
            raise

def lambda_handler(event, context):
    """Lambdaå¤„ç†å‡½æ•°"""
    try:
        logger.info("Lambdaå‡½æ•°å¼€å§‹æ‰§è¡Œ")
        
        if 'Records' not in event or not event['Records']:
            logger.error("äº‹ä»¶ä¸­æ²¡æœ‰Recordså­—æ®µæˆ–Recordsä¸ºç©º")
            return {'statusCode': 400, 'body': 'Invalid event structure'}
            
        for record in event['Records']:
            request_id = record['messageId']
            logger.info(f"å¼€å§‹å¤„ç†æ¶ˆæ¯ - RequestID: {request_id}")
            
            try:
                # æ£€æŸ¥å‰©ä½™æ‰§è¡Œæ—¶é—´
                remaining_time = context.get_remaining_time_in_millis() / 1000
                if remaining_time < Config.MIN_REMAINING_TIME:
                    logger.warning(
                        f"å‰©ä½™æ‰§è¡Œæ—¶é—´ä¸è¶³: {remaining_time}ç§’ï¼Œ"
                        f"éœ€è¦è‡³å°‘{Config.MIN_REMAINING_TIME}ç§’"
                    )
                    continue
                
                message = json.loads(record['body'])
                
                plots = message['plots']
                time_window = message['time_window_hours']
                source_csv = message['source_csv']
                
                metric_type = message.get('metric_type') or get_metric_type(source_csv)
                if not metric_type:
                    raise ValueError(f"æ— æ³•ç¡®å®šæŒ‡æ ‡ç±»å‹: {source_csv}")

                logger.info(f"å¤„ç†æŒ‡æ ‡ç±»å‹: {metric_type}")
                
                # è°ƒç”¨Difyåˆ†æ
                dify_client = DifyClient()
                analysis_results = dify_client.analyze_plots(plots, metric_type)
                
                if analysis_results:
                    # å‘é€Larkæ¶ˆæ¯
                    logger.info("å‘é€åˆ†æç»“æœåˆ°Lark")
                    lark_bot = LarkBot()
                    lark_bot.send_message(
                        analysis_results,
                        source_csv,
                        time_window,
                        metric_type
                    )
                else:
                    logger.info("æœªå‘ç°å¼‚å¸¸,è·³è¿‡å‘é€æ¶ˆæ¯")
                
                logger.info("æ¶ˆæ¯å¤„ç†å®Œæˆ")

            except Exception as e:
                logger.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                continue

        return {
            'statusCode': 200,
            'body': 'Successfully processed all records'
        }

    except Exception as e:
        logger.error(f"Lambdaæ‰§è¡Œå¤±è´¥: {str(e)}")
        return {
            'statusCode': 500,
            'body': f'Lambda execution failed: {str(e)}'
        }